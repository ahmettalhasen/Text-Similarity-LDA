{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projectWork.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmettalhasen/Text-Similarity-LDA/blob/master/projectWork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5_2dU4hQEU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk import FreqDist\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from scipy.stats import entropy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "import gensim\n",
        "from gensim.models import LdaModel\n",
        "from gensim import models, corpora, similarities\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "!pip install pyLDAvis #Visualize the topics of LDA model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c4EkSx3P5Zb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Importing the dataset\n",
        "df = pd.read_csv('gdrive/My Drive/summer2019/cleanData.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5mRMvTsQDf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['tokenized'] = [[word for word in line.split(',') if len(word) > 1] for line in df['words']]\n",
        "df = df.drop(columns = ['question', 'details', 'quest_len', 'words'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pVtKGfc7hpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp80cjN0zCAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Using FreqDist\n",
        "words = [word for row in list(df.tokenized) for word in row]\n",
        "\n",
        "freqDist = FreqDist(words)\n",
        "print(\"Number of distinct words is, \", len(freqDist))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwh-JO3yzGyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Top 30 used words\n",
        "k = 10000\n",
        "top_k_words = freqDist.most_common(k)\n",
        "top_k_words[:500]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qu2Wm1pYzIyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Bottom 30 used words\n",
        "k = 3600\n",
        "top_k_words = freqDist.most_common(k)\n",
        "top_k_words[-100:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iEZHCFBzLn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_k_words,_ = zip(*freqDist.most_common(k))\n",
        "top_k_words = set(top_k_words)\n",
        "#Throws all the rare used words away\n",
        "def keep_top_k_words(text):\n",
        "  \n",
        "    return [word for word in text if word in top_k_words]\n",
        "\n",
        "df['tokenized'] = df['tokenized'].apply(keep_top_k_words)\n",
        "df = df.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz9u_NRzzZa6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['quest_len'] = df['tokenized'].apply(lambda x: len(x))\n",
        "quest_lengths = list(df['quest_len'])\n",
        "\n",
        "MIN_TOKEN_NUMBER = 4\n",
        "df = df[df['quest_len'] > MIN_TOKEN_NUMBER]\n",
        "\n",
        "# Histogram Plot\n",
        "fig, ax = plt.subplots(figsize=(16,8));\n",
        "n, bins, patches = ax.hist(quest_lengths, 750)\n",
        "ax.set_xlabel('Question Length (tokens)', fontsize=20)\n",
        "ax.set_ylabel('Count', fontsize=20)\n",
        "ax.grid()\n",
        "plt.xlim(0,150)\n",
        "ax.grid()\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-gPT1JpzkN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.reset_index(drop=True)\n",
        "df.drop(labels='quest_len', axis=1, inplace=True)\n",
        "\n",
        "print(\"length of list:\",len(quest_lengths),\n",
        "      \"\\naverage question length\", np.average(quest_lengths),\n",
        "      \"\\nminimum question length\", min(quest_lengths),\n",
        "      \"\\nmaximum question length\", max(quest_lengths))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_9t_gopejeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Splitting into train and test sets.\n",
        "chosen = np.random.rand(len(df)) < 0.9999\n",
        "train_set = df[chosen]\n",
        "test_set = df[~chosen]\n",
        "train_set.reset_index(drop=True,inplace=True)\n",
        "test_set.reset_index(drop=True,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW63Wv988RCs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOhmLYg-f-G7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating corpus an dictionary\n",
        "\n",
        "dictionary = corpora.Dictionary(train_set['tokenized'])\n",
        "corpus = [dictionary.doc2bow(doc) for doc in train_set['tokenized']]\n",
        "\n",
        "def train_lda(cluster_size, num_passes):\n",
        "    \"\"\"\n",
        "    This function trains the LDA model\n",
        "    We setup parameters like number of topics, number of passes to train the model \n",
        "    and train set\n",
        "    \"\"\"\n",
        "    \n",
        "    time01 = time.time()\n",
        "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
        "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
        "    lda = LdaModel(corpus=corpus, num_topics=cluster_size, id2word=dictionary,\n",
        "                   alpha=1e-2, eta=0.5e-2, minimum_probability=0.0, passes=num_passes)\n",
        "    time02 = time.time()\n",
        "    print(\"Time to train LDA model on \", len(df), \"questions: \", (time02-time01)/60, \"min\")\n",
        "    return lda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saVUH30eOf9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", category=PendingDeprecationWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-9wXuH6m4FL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training LDA Model and displaying the characteristic words\n",
        "num_clusters = 250\n",
        "lda = train_lda(num_clusters, 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyipJIZJMYBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving Model to Google Drive\n",
        "lda.save('gdrive/My Drive/models/model250.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_ZkGQQ24E4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Retrieving Model from Google Drive\n",
        "#lda = lda.load('gdrive/My Drive/models/model500.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkrJiG96b2hM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "topic_highlights = [('*---------------------------', lda.show_topic(topicid=i, topn=7)) for i in range(num_clusters) ]\n",
        "topic_highlights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp9gYbtw5CT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#lda.show_topics(250,5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGLt9l5HCw_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Evaluation of the number of the topics\n",
        "import pyLDAvis.gensim\n",
        "\n",
        "LDAvis_prepared = pyLDAvis.gensim.prepare(lda, corpus, dictionary, mds='mmds')\n",
        "pyLDAvis.display(LDAvis_prepared)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k89u78hqjxv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Topic Distribution\n",
        "\n",
        "def calculate_all_documents_distribution(ldaModel):\n",
        "    \"\"\"\n",
        "    This function calculates the topic distributions of the all quesitons in the train set\n",
        "    \"\"\"\n",
        "    docs_topic_dist = np.array([[tup[1] for tup in lst] for lst in ldaModel[corpus]])\n",
        "    return docs_topic_dist\n",
        "\n",
        "\n",
        "docs_topic_dist = calculate_all_documents_distribution(lda)\n",
        "print(\"Shape of the matrix is \", docs_topic_dist.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9S79E9RjBAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Getting a Question From the Test Set\n",
        "def random_question_picker(test_set):\n",
        "    \"\"\"\n",
        "    This function picks a random question from test set \n",
        "    \"\"\"\n",
        "    random_article_index = np.random.randint(len(test_set))\n",
        "    return random_article_index \n",
        "  \n",
        "def calculate_new_document_distribution(test_set, random_article_index):\n",
        "    \"\"\"\n",
        "    This function calculates the topic distributions of the given question\n",
        "    \"\"\"\n",
        "    new_bow = dictionary.doc2bow(test_set.tokenized[random_article_index])\n",
        "    print(test_set.quest[random_article_index])\n",
        "    new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])\n",
        "    return new_doc_distribution"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D8YeZr1j4b_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jensen_shannon(query, matrix):\n",
        "    \"\"\"\n",
        "    This function implements a Jensen-Shannon similarity\n",
        "    between the input query (an LDA topic distribution for a document)\n",
        "    and the entire corpus of topic distributions.\n",
        "    It returns an array of length M where M is the number of documents in the corpus\n",
        "    \"\"\"\n",
        "    # lets keep with the p,q notation above\n",
        "    p = query[None,:].T # take transpose\n",
        "    q = matrix.T # transpose matrix\n",
        "    m = 0.5*(p + q)\n",
        "    return np.sqrt(0.5*(entropy(p,m) + entropy(q,m)))\n",
        "  \n",
        "def cosine_dist(query, matrix):\n",
        "    \"\"\"\n",
        "    This function implements cosine similarity\n",
        "    between the input query (an LDA topic distribution for a document)\n",
        "    and the entire corpus of topic distributions.\n",
        "    It returns an array of length M where M is the number of documents in the corpus\n",
        "    \"\"\"\n",
        "    return [gensim.matutils.cossim(query, x) for x in matrix]\n",
        "  \n",
        "\n",
        "def get_most_similar_documents(query,matrix,k=10):\n",
        "    \"\"\"\n",
        "    This function implements the Jensen-Shannon distance above\n",
        "    and retruns the top k indices of the smallest jensen shannon distances\n",
        "    \"\"\"\n",
        "    sims = jensen_shannon(query,matrix) # list of jensen shannon distances\n",
        "    return sims.argsort()[:k] # the top k positional index of the smallest Jensen Shannon distances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmaB2bwPj5Qz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def retrieve_similar_questions(test_set, train_set, random_article_index, docs_topic_dist, num_reccom=10):\n",
        "    \"\"\"\n",
        "    This functions retrives similar questions from the train set\n",
        "    \"\"\"\n",
        "    new_doc_distribution = calculate_new_document_distribution(test_set, random_article_index)\n",
        "    most_sim_ids = get_most_similar_documents(new_doc_distribution,docs_topic_dist, num_reccom)\n",
        "    most_similar_df = train_set[train_set.index.isin(most_sim_ids)]\n",
        "    questList = list(most_similar_df['quest'])\n",
        "    tokenizedList = list(most_similar_df['tokenized'])\n",
        "    idList = list(most_similar_df['id'])\n",
        "    return questList,tokenizedList,idList"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72hMsve4oxbn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Giving it a try\n",
        "def try_many(number):\n",
        "    for i in range(number):\n",
        "      print(\"The Question Number \", i)\n",
        "      qList,tokenizedList,idList = retrieve_similar_questions(test_set, train_set, i, docs_topic_dist)\n",
        "      a = [print(\"Similar--> \", el) for el in qList ]\n",
        "\n",
        "try_many(len(test_set))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLlTyuqyc5lK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", category=PendingDeprecationWarning)\n",
        "start = time.time()\n",
        "import os\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "if not os.path.exists('gdrive/My Drive/summer2019/GoogleNews-vectors-negative300.bin.gz'):\n",
        "    raise ValueError(\"SKIP: You need to download the google news model\")\n",
        "model = KeyedVectors.load_word2vec_format('gdrive/My Drive/summer2019/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "\n",
        "print('Cell took %.2f seconds to run.' % (time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S5XaCD5afQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numX = 14\n",
        "qList,tokenizedList,idList = retrieve_similar_questions(test_set, train_set, numX, docs_topic_dist, 1000)\n",
        "vec = {'Id':idList, 'Question':qList, 'Tokenized':tokenizedList}\n",
        "rec_dataframe = pd.DataFrame(vec)\n",
        "rec_dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljKZYeg6pniV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "time001 = time.time()\n",
        "rec_dataframe = rec_dataframe.reset_index(drop=True)\n",
        "i = 0\n",
        "dist = [model.wmdistance(sen, test_set.tokenized[numX]) for sen in rec_dataframe.Tokenized]\n",
        "       \n",
        "rec_dataframe['dist'] = dist\n",
        "time002 = time.time()\n",
        "print(\"Finding the closest answers took \",(time002 - time001)/60, \" min\" )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YjT3gNNrpDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res = rec_dataframe.sort_values('dist')\n",
        "res = res.reset_index(drop=True)\n",
        "res\n",
        "for i in range(10):\n",
        "  print(res.Question[i])\n",
        "res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn8E09z7gc_U",
        "colab_type": "text"
      },
      "source": [
        "Project ends here, the below part ara some alternatives or looking from different perspective."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGagfFxNpUjz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_recommendation_csv(train_set, test_set):\n",
        "    \"\"\"\n",
        "    this function finds recommendation for multiple test data and saving them in a csv file\n",
        "    \"\"\"\n",
        "    dataframe = pd.DataFrame(columns=['id', 'question', 'recommendations', 'idsOfRecommendations'])\n",
        "    \n",
        "    for i in range(len(test_set)):\n",
        "      question_index = i\n",
        "      qList, idList = retrieve_similar_questions(test_set, train_set, question_index, docs_topic_dist)\n",
        "      text = \"\"\n",
        "      ids = \"\"\n",
        "      for j in range(len(qList)):\n",
        "        text = text + \"|\" + qList[j]\n",
        "        ids = ids + \",\" + str(idList[j])\n",
        "        \n",
        "      dataframe.loc[i] = [str(test_set['id'][i]), test_set['quest'][i], text, ids]     \n",
        "      \n",
        "    ## Saving the latest verison as csv for practical use\n",
        "    export_csv = dataframe.to_csv('gdrive/My Drive/summer2019/testRecommendations.csv', index = None, header=True)\n",
        "    print(\"testRecommendations.csv is created.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHeVTa59xKsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_recommendation_csv(train_set, test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZGzlwHZ7vDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = pd.read_csv('gdrive/My Drive/summer2019/testRecommendations.csv')\n",
        "d.recommendations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4EuLyRf2tlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ques = []\n",
        "for row in train_set['tokenized']:\n",
        "  row = list(dict.fromkeys(row))\n",
        "  s = \"\"\n",
        "  for x in row:\n",
        "    s = s + \" \" + x\n",
        "  s = s.strip()\n",
        "  ques.append(s)\n",
        "  \n",
        "ques"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgKpz6an2dHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################\n",
        "\n",
        "\n",
        "from nltk import ngrams\n",
        "from scipy.spatial import distance\n",
        "\n",
        "def similar_question_jaccard(question, train_set):\n",
        "    \"\"\"\n",
        "    This function to finds and returns similar questions among train set \n",
        "    by Jaccard Distance\n",
        "    \"\"\"\n",
        "    questions=[]\n",
        "    for train_quest in train_set:\n",
        "        dist=distance.jaccard(set(ngrams(question,n=1)),set(ngrams(train_quest,n=1)))\n",
        "        if(dist<.1):\n",
        "            questions.append(train_quest)\n",
        "    return questions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md7gl1jq0J1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quesTest = ques[:15]\n",
        "quesTrain = ques[15:]\n",
        "\n",
        "for q in quesTest:\n",
        "  print(\"The Question --------> \", q)\n",
        "  qs = similar_question_jaccard(q,quesTrain)\n",
        "  v = [print(\"Similar ---> \", s) for s in qs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_VX125XNnxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "randNum = random_question_picker(test_set)\n",
        "test_set.quest[randNum]\n",
        "qs = similar_question_jaccard(test_set.tokenized[randNum])\n",
        "qs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5TtBwIdCDU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#word2Vec\n",
        "model = word2vec.Word2Vec(corpus, size=100, window=20, min_count=40, workers=4)\n",
        "#TSNE\n",
        "\n",
        "\n",
        "def tsne_plot(model,title='None'):\n",
        "    \"\"\"\n",
        "    This function creates and TSNE model and plots it\n",
        "    \"\"\"\n",
        "    labels = []\n",
        "    tokens = []\n",
        "\n",
        "    for word in model.wv.vocab:\n",
        "        tokens.append(model[word])\n",
        "        labels.append(word)\n",
        "    \n",
        "    tsne_model = TSNE(perplexity=80, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
        "    new_values = tsne_model.fit_transform(tokens)\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    for value in new_values:\n",
        "        x.append(value[0])\n",
        "        y.append(value[1])\n",
        "        \n",
        "    plt.figure(figsize=(12, 12)) \n",
        "    plt.title(title)\n",
        "    for i in range(len(x)):\n",
        "        plt.scatter(x[i],y[i])\n",
        "        plt.annotate(labels[i],\n",
        "                     xy=(x[i], y[i]),\n",
        "                     xytext=(5, 2),\n",
        "                     textcoords='offset points',\n",
        "                     ha='right',\n",
        "                     va='bottom')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "tsne_plot(lda,'Questions')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSDsjPDsklrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now, We will analyze the goodness of our model based on number of clusters by using coherence value\n",
        "def compute_coherence_values(train_set, stop=100, start=20, step=10):\n",
        "    \"\"\"\n",
        "    Input   : dictionary : Gensim dictionary\n",
        "              corpus : Gensim corpus\n",
        "              texts : List of input texts\n",
        "              stop : Max num of topics\n",
        "    purpose : Compute c_v coherence for various number of topics\n",
        "    Output  : model_list : List of LSA topic models\n",
        "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    timeCohSt = time.time()\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    \n",
        "    for num_topics in range(start, stop, step):\n",
        "        # generate LDA model\n",
        "        timeTrain01 = time.time()\n",
        "        model = train_lda(num_topics, 5, train_set)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=train_set, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "        timeTrain02 = time.time()\n",
        "        print(\"LDA Model with \", num_topics, \" topics trained in \", \n",
        "              (timeTrain02-timeTrain01)/60, \" min. The coherence value which is \",\n",
        "              coherencemodel.get_coherence(), \" added!\") \n",
        "    \n",
        "    timeCohEnd = time.time()\n",
        "    print(\"Time to train all LDA models and calculating coherence took  \", (timeCohEnd-timeCohSt)/60, \"min\")\n",
        "    return model_list, coherence_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qMxN7sHoYRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start,stop,step=30,110,10\n",
        "model_list, coherence_values = compute_coherence_values(train_set, stop, start, step)\n",
        "\n",
        "# Show graph\n",
        "plt.plot(range(start, stop, step), coherence_values)\n",
        "plt.xlabel(\"Number of Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}