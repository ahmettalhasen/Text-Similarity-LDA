{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projectSupportCleaning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmettalhasen/Text-Similarity-LDA/blob/master/projectSupportCleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcvQXnsGpVAk",
        "colab_type": "code",
        "outputId": "219c81a2-6d91-4c33-fe97-3f14867bf0fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import gensim\n",
        "from gensim.models import LdaModel\n",
        "from gensim import models, corpora, similarities\n",
        "import re\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import time\n",
        "from nltk import FreqDist\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from scipy.stats import entropy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "!pip install pyspellchecker # For correcting the spell mistakes\n",
        "from spellchecker import SpellChecker\n",
        "!pip install langdetect\n",
        "from langdetect import detect \n",
        "from langdetect.lang_detect_exception import LangDetectException"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (1.0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cm-WTedsiHx",
        "colab_type": "code",
        "outputId": "e01fa6b8-14c8-4d26-8c78-8ca11168f44d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Importing the dataset\n",
        "df = pd.read_csv('gdrive/My Drive/summer2019/support_forum_questions.csv',sep=\"|\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2L-gumIuRKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Replacing Nans in 'tags' with \n",
        "# df.loc[df.tags.isnull()]['tags'] = ' '"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62QdUZorui66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.dropna(axis = 0, inplace = True)\n",
        "df.isnull().sum()\n",
        "\n",
        "df = df.sample(frac=1.0)\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "df = df.drop('login', axis=1)\n",
        "df = df.drop('added', axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXaINw450vYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "#Clearing Html function\n",
        "def clearhtml(raw_html):\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, ' ', raw_html)\n",
        "    cleantext = re.sub('\\n', ' ', cleantext)\n",
        "    cleantext = re.sub('\\r', ' ', cleantext)\n",
        "    cleantext = re.sub('&nbsp', ' ', cleantext)\n",
        "    return cleantext\n",
        "\n",
        "def initial_clean(text):\n",
        "    \"\"\"\n",
        "    Function that cleans emails, websites and any symbols/punctuations\n",
        "    \"\"\"\n",
        "    text = re.sub(\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\"jotform\", \" form \", text)\n",
        "    text = re.sub(\"[^a-zA-Z ]\", \" \", text)\n",
        "    text = text.lower() \n",
        "    return text\n",
        "\n",
        "def first_preprocess(text):\n",
        "    \"\"\"\n",
        "    Function that applies clearhtml and initial_clean\n",
        "    \"\"\"\n",
        "    return initial_clean(clearhtml(text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyFPS5R8KwPl",
        "colab_type": "code",
        "outputId": "7b554da5-1f70-4f80-a7ae-7fd6fc85ad2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Preprocessing\n",
        "t1 = time.time()\n",
        "df['question'] = df['question'].apply(first_preprocess)\n",
        "df['details'] = df['details'].apply(first_preprocess)\n",
        "df['quest'] = df['question'] + ' ' + df['details']\n",
        "#Deleting the questions less than 30 character size --> They are all test entries or spams\n",
        "df = df[df.quest.str.len() > 30 ]\n",
        "t2 = time.time()\n",
        "print(\"Time to clean Html\", len(df), \"articles:\", (t2-t1)/60, \"min\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to clean Html 227526 articles: 28.61263000567754 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV6fA3dnMhz9",
        "colab_type": "code",
        "outputId": "d0d15df3-6ac2-4e4a-a211-30feaed01d36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Filtering the non-english questions out\n",
        "\n",
        "def filter_language(text):\n",
        "    \"\"\"\n",
        "    Function that applies all three functions abov\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except LangDetectException:\n",
        "        pass\n",
        "      \n",
        "t3 = time.time()     \n",
        "df = df[df.quest.apply(filter_language) == 'en']\n",
        "t4 = time.time()\n",
        "print(\"Time to filtering non-english questions took \", (t4-t3)/60, \"min\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to filtering non-english questions took  21.599160651365917 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNHCFPY_MfiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Getting text tokenized and ready to process ###\n",
        "\n",
        "def tokenizer(text):\n",
        "    text = nltk.word_tokenize(text)\n",
        "    return text  \n",
        "  \n",
        "stop_words = stopwords.words('english')\n",
        "def remove_stop_words_and_junk(text):\n",
        "    \"\"\"\n",
        "    Function that removes all stopwords and undesired ones from text\n",
        "    \"\"\"\n",
        "    #Deleting undesired words\n",
        "    undesiredWords = ['would', 'hi', 'hello', 'thank', 'ive', 'havent', 'hasnt', \n",
        "                  'hadnt', 'arent', 'isnt', 'wouldnt', 'dont', 'werent', \n",
        "                  'couldnt', 'wont', 'cant', 'didnt', \"doesnt\", 'without',\n",
        "                  'please','thanks', 'could']\n",
        "    undesiredWords = set(undesiredWords)\n",
        "    \n",
        "    return [word for word in text if word not in stop_words and word not in undesiredWords]\n",
        "\n",
        "def second_preprocess(text):\n",
        "    \"\"\"\n",
        "    Function that tokenizes and removes stop words and junk some words\n",
        "    \"\"\"\n",
        "    return remove_stop_words_and_junk(tokenizer(text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OdJp2nOLjcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t5 = time.time()\n",
        "df['tokenized'] = df['quest'].apply(second_preprocess)\n",
        "t6 = time.time()\n",
        "print(\"Time to tokenize and perfom the removals for\", len(df), \"questions took \", (t6-t5)/60, \"min\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQMEj4GbGyDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = [word for row in list(df.tokenized) for word in row]\n",
        "freqDist = FreqDist(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOfThNs94kP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNHZYgaOG7zz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spell = SpellChecker() \n",
        "def spelling_mistake_corrector(word):\n",
        "    \"\"\"\n",
        "    Function that corrects the spelling mistake.\n",
        "    Corrects if the number of occurences of the correct form is greater than\n",
        "    the number of occurences of the original form in order to prevent miscorrection\n",
        "    of some words.\n",
        "    \"\"\"\n",
        "    checkedWord = spell.correction(word)\n",
        "    if freqDist[checkedWord] >= freqDist[word]:\n",
        "        word = checkedWord\n",
        "    return word\n",
        "  \n",
        "def correctorForAll(text):\n",
        "    \"\"\"\n",
        "    Function that applies spelling_mistake_corrector to all words\n",
        "    \"\"\"\n",
        "    text = [spelling_mistake_corrector(word) for word in text]\n",
        "    return text\n",
        "\n",
        "#Option 1\n",
        "stemmer = PorterStemmer()\n",
        "def stem_words(text):\n",
        "    \"\"\"\n",
        "    Function to stem words, so all forms of a word is treated in the same way \n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = [stemmer.stem(word) for word in text]\n",
        "        text = [word for word in text if len(word) > 2] #filtering 1 and 2 letter words out\n",
        "    except IndexError:\n",
        "        pass\n",
        "    return text\n",
        "\n",
        "#Option2\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_words(text):\n",
        "    \"\"\"\n",
        "    Function to lemmatize words.\n",
        "    \"\"\"\n",
        "    text = [lemmatizer.lemmatize(word) for word in text]\n",
        "    text = [word for word in text if len(word) > 2] #filtering 1 and 2 letter words out\n",
        "    return text\n",
        "\n",
        "def apply_corrector_and_lemmatizer(text):\n",
        "    \"\"\"\n",
        "    This function applies all the functions above \n",
        "    \"\"\" \n",
        "    return lemmatize_words(correctorForAll(text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYuCvbaNHcCg",
        "colab_type": "code",
        "outputId": "0801d2c0-e9c5-4dfd-cbe9-07c144abf5d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "t7 = time.time()\n",
        "df['tokenized'] = df['tokenized'].apply(stem_words)\n",
        "t8 = time.time()\n",
        "print(\"Time to stem words for \", len(df), \" questions took\", (t8-t7)/60, \"min\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to correct and lemmatize for  201609  questions took 2.9812321186065676 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5j_2vVwP88Y",
        "colab_type": "code",
        "outputId": "79b81b1c-9e91-4ddf-fb3f-9f40a2e44182",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "t9 = time.time()\n",
        "df['quest_len'] = df['tokenized'].apply(lambda x: len(x))\n",
        "MIN_TOKEN_NUMBER = 9\n",
        "df = df[df['quest_len'] > MIN_TOKEN_NUMBER]\n",
        "t10 = time.time()\n",
        "print(\"Time to drop the questions with few words took \", (t10-t9)/60, \"min\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to drop the questions with few words took  0.003864320119222005 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr1VB7TbgAsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kcT_r5V0vDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def csv_formater(line):\n",
        "    strr = \"\"\n",
        "    for word in line:\n",
        "        strr = strr + word + \",\"\n",
        "    return strr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xN4BsgLYIfH1",
        "colab_type": "code",
        "outputId": "7b1423ae-ebb5-4838-a6e1-fd46bc50da6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "t11 = time.time()\n",
        "df['words'] = df['tokenized'].apply(csv_formater)\n",
        "t12 = time.time()\n",
        "print(\"Time to make the file ready to store took \", (t12-t11)/60, \"min\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to make the file ready to store took  0.01765124797821045 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvtjXaQgPjOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8AdiuTJh6MM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Saving the latest verison as csv for practical use\n",
        "export_csv = df.to_csv ('gdrive/My Drive/summer2019/cleanData.csv', index = None, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m61kUGydif6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = pd.read_csv('gdrive/My Drive/summer2019/cleanData.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAcsFfgUXyVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}