{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projectSupportCleaning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmettalhasen/Text-Similarity/blob/master/projectSupportCleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcvQXnsGpVAk",
        "colab_type": "code",
        "outputId": "3f1aa659-2700-4a9c-8c51-7aac0d861656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import gensim\n",
        "from gensim.models import LdaModel\n",
        "from gensim import models, corpora, similarities\n",
        "import re\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import time\n",
        "from nltk import FreqDist\n",
        "from scipy.stats import entropy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cm-WTedsiHx",
        "colab_type": "code",
        "outputId": "a4468ec4-3eee-4463-b063-97c8cd409683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Importing the dataset\n",
        "df = pd.read_csv('gdrive/My Drive/summer2019/support_forum_questions.csv',sep=\"|\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2L-gumIuRKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Replacing Nans in 'tags' with \n",
        "# df.loc[df.tags.isnull()]['tags'] = ' '"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62QdUZorui66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.dropna(axis = 0, inplace = True)\n",
        "df.isnull().sum()\n",
        "\n",
        "df = df.sample(frac=1.0)\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "df = df.drop('login', axis=1)\n",
        "df = df.drop('added', axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXaINw450vYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "#Clearing Html function\n",
        "def clearhtml(raw_html):\n",
        "   cleanr = re.compile('<.*?>')\n",
        "   cleantext = re.sub(cleanr, ' ', raw_html)\n",
        "   return cleantext\n",
        "\n",
        "#Clearing all the irrelevant things\n",
        "def clearstuff(text):\n",
        "  cleantext = re.sub('\\n', ' ', text)\n",
        "  cleantext = re.sub('\\r', ' ', cleantext)\n",
        "  cleantext = re.sub('&nbsp', ' ', cleantext)\n",
        "  return cleantext\n",
        "\n",
        "#def clearIRwords(text):\n",
        "#  cleantext = re.sub('hello', ' ', text)\n",
        "#  cleantext = re.sub('thanks', ' ', cleantext)\n",
        "#  cleantext = re.sub('thank you', ' ', cleantext)\n",
        "#  cleantext = re.sub('hi(\\s)', ' ', cleantext)\n",
        "#  cleantext = re.sub('aytekin', ' ', cleantext)\n",
        "#  return cleantext\n",
        "\n",
        "def initial_clean(text):\n",
        "    \"\"\"\n",
        "    Function that cleans emails, websites and any symbols/punctuations\n",
        "    \"\"\"\n",
        "    text = re.sub(\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(\"[^a-zA-Z ]\", \" \", text)\n",
        "    text = text.lower() \n",
        "    return text\n",
        "\n",
        "def preprocess(text):\n",
        "  text = initial_clean(clearstuff(clearhtml(text)))\n",
        "  return text\n",
        "\n",
        "\n",
        "### Getting text tokenized and ready to process ###\n",
        "\n",
        "def tokenizer(text):\n",
        "    text = nltk.word_tokenize(text)\n",
        "    return text  \n",
        "  \n",
        "stop_words = stopwords.words('english')\n",
        "def remove_stop_words(text):\n",
        "    \"\"\"\n",
        "    Function that removes all stopwords from text\n",
        "    \"\"\"\n",
        "    return [word for word in text if word not in stop_words]\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "def stem_words(text):\n",
        "    \"\"\"\n",
        "    Function to stem words, so plural and singular are treated the same\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = [stemmer.stem(word) for word in text]\n",
        "        text = [word for word in text if len(word) > 1] # make sure we have no 1 letter words\n",
        "    except IndexError: # the word \"oed\" broke this, so needed try except\n",
        "        pass\n",
        "    return text\n",
        "\n",
        "def apply_all(text):\n",
        "    \"\"\"\n",
        "    This function applies all the functions above into one *\n",
        "    \"\"\" \n",
        "    return stem_words(remove_stop_words(tokenizer(text)))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru1RxqZ1IRb9",
        "colab_type": "code",
        "outputId": "33a68c99-afc0-498b-be4a-f2c8ea6360e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Clearing html and updating data frame\n",
        "t1 = time.time()\n",
        "df['question'] = df['question'].apply(preprocess)\n",
        "df['details'] = df['details'].apply(preprocess)\n",
        "t2 = time.time()\n",
        "print(\"Time to clean Html\", len(df), \"articles:\", (t2-t1)/60, \"min\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to clean Html 227934 articles: 27.781556793053944 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82NdYuJkLW6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['quest'] = df['question'] + ' ' + df['details']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARqeXxhiUr5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Deleting the questions less tah. 30 character size -- They are all test entries or spams\n",
        "df = df[df.quest.str.len() > 30 ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUeRzPcASLDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Copying the dataframe just in case\n",
        "dfCopy = df.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbmNjrOnQi5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install langdetect\n",
        "from langdetect import detect \n",
        "from langdetect.lang_detect_exception import LangDetectException\n",
        "\n",
        "def filter_language(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except LangDetectException:\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xJy2c5XWzE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Filtering the non-english questions out\n",
        "dfEx = df[df.quest.apply(filter_language) == 'en']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr1VB7TbgAsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = dfEx\n",
        "df = df.reset_index(drop=True)\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kcT_r5V0vDW",
        "colab_type": "code",
        "outputId": "e36fab41-41db-4e72-fa92-55893e843dcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Clearing punctuation, tokenizing the words and dropping the stop_words\n",
        "t3 = time.time()\n",
        "df['tokenized'] = df['quest'].apply(apply_all)\n",
        "t4 = time.time()\n",
        "print(\"Time to clean and tokenize\", len(df), \"questions:\", (t4-t3)/60, \"min\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to clean and tokenize 220421 articles: 5.324121356010437 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8AdiuTJh6MM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Saving the latest verison as csv for practical use\n",
        "export_csv = df.to_csv ('gdrive/My Drive/summer2019/cleanSupportData.csv', index = None, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m61kUGydif6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d = pd.read_csv('gdrive/My Drive/summer2019/cleanSupportData.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}